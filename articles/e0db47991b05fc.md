---
title: "Batch Normalization 勉強の備忘録"
emoji: "⛳"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

### どんなものであるか
入力となる特徴量のみを正規化するのではなく、ミニパッチごとの統計量を使用してレイヤごとに正規化をする。


### なぜ必要なのか
Convariate Shift を解決したい．
    Convariate Shift とは，学習データとテストデータとで特徴量の分布が異なる場合に，学習データに多い特徴量の値の領域ではテストデータの評価もうまくできるが，学習データに少ない特徴量の値の領域では，テストデータをうまく評価することができないこと．



### どこがすごいのか
- 学習が安定する
- 学習スピードが速くなる
- dropoutの必要性を減らすことができる
- 学習率を大きくすることができる


### キモの部分はどこか
レイヤの入力はパラメータが変化するごとに値が変化する．つまり，パラメータを更新するたびに学習データ全体をネットワークに流して，各レイヤの入力を求め，正規化しなければならない．

↑効率がよくない．．．　そこで！

Batch Normalizationでは，全学習データを使用するのではなく，ミニバッチごとの統計量を使って，ミニバッチごとに正規化を行う！
学習時とテスト時では少し異なる計算を行っている．
学習時は，ランダムなミニバッチ内を毎回正規化し，その各隠れ層の出力の暫定的な正則化結果によって，学習していたが，テスト時にはミニバッチを得ることができない．よって，データセット全体からあらかじめ決定して置いた平均・分散を常に用いて正規化を行う必要がある．

CNNなどのDNNの繰り返し構造は，　　畳み込み層　⇒　活性化関数　⇒　プーリング層　の3種類であったが，
バッチ正則化の登場以降は，　　畳み込み層　⇒　バッチ正則化　⇒　活性化関数　⇒　プーリング層　の4種類



### 議論はあるのか
- バッチサイズが小さいとうまく学習できない
- バッチサイズが大きいと正規化の効果が減る


### 付随して調べるべきこと
非線形性の良さを完全に理解しきれていないですね．．．
レイヤ正則化・グループ正則化・インスタンス正則化も

参考
https://arxiv.org/abs/1502.03167